{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf1b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.tokenize import word_tokenize\n",
    "from torch.autograd import Variable\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "def make_sents(sp_matrix):\n",
    "    sent = []\n",
    "    unfort_list = []\n",
    "    ii =0\n",
    "    for i in range(sp_matrix.shape[0]):  \n",
    "\n",
    "        pd_table_sub = sp_matrix.iloc[i,:]>0\n",
    "        #pd_table_sub_num = pd_table_sub[pd_table_sub>0]\n",
    "        c_v = sp_matrix.iloc[i,:][sp_matrix.iloc[i,:]>0]#.index#np.unique(pd_table_sub.Species.dropna())\n",
    "        #subsent =[]\n",
    "        if len(c_v)>1:\n",
    "            #for ii in range(10):\n",
    "            sent.append(c_v)#tolist\n",
    "        else:\n",
    "            unfort_list.append(ii)\n",
    "        ii += 1\n",
    "        print('\\r  %s...'%ii,end = '')\n",
    "    return(sent)\n",
    "\n",
    "\n",
    "\n",
    "def glove_dic(sent,embed_size,vocab,wf,num_epochs = 5,glove_abuna = True):\n",
    "    # Set parameters\n",
    "    xmax = 2\n",
    "    alpha = 0.75\n",
    "    batch_size = 100\n",
    "    l_rate = 0.001\n",
    "    num_epochs = num_epochs\n",
    "\n",
    "    # Create vocabulary and word lists\n",
    "    word_list = sent\n",
    "    w_list_size = len(word_list)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Create word to index mapping\n",
    "    w_to_i = {word: ind for ind, word in enumerate(vocab)}\n",
    "    comat = np.zeros((vocab_size, vocab_size))\n",
    "    if glove_abuna == True:\n",
    "        # Construct co-occurence matrix\n",
    "        for i_init in range(w_list_size):\n",
    "            cc = sent[i_init]\n",
    "            for i in range(len(cc)):\n",
    "                for j in range(len(cc)):\n",
    "                    if i != j:\n",
    "                        ind = w_to_i[cc.index[i]]\n",
    "                        lind = w_to_i[cc.index[j]]\n",
    "                        comat[ind, lind] += cc.values[i]\n",
    "                        comat[lind,ind] += cc.values[j]\n",
    "\n",
    "    # Non-zero co-occurrences\n",
    "    else:\n",
    "        for i_init in range(w_list_size):\n",
    "            cc = sent[i_init]\n",
    "            for i in range(len(cc)):\n",
    "                for j in range(len(cc)):\n",
    "                    if i != j:\n",
    "                        ind = w_to_i[cc.index[i]]\n",
    "                        lind = w_to_i[cc.index[j]]\n",
    "                        comat[ind, lind] += 1\n",
    "                        comat[lind,ind] += 1\n",
    "\n",
    "    coocs = np.transpose(np.nonzero(comat))\n",
    "    # Weight function\n",
    "    # Set up word vectors and biases\n",
    "    l_embed, r_embed = [\n",
    "\t[Variable(torch.from_numpy(np.random.normal(0, 0.01, (embed_size, 1))),\n",
    "\t\trequires_grad = True) for j in range(vocab_size)] for i in range(2)]\n",
    "    l_biases, r_biases = [\n",
    "        [Variable(torch.from_numpy(np.random.normal(0, 0.01, 1)), \n",
    "            requires_grad = True) for j in range(vocab_size)] for i in range(2)]\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = optim.Adam(l_embed + r_embed + l_biases + r_biases, lr = l_rate)\n",
    "\n",
    "    # Batch sampling function\n",
    "    def gen_batch():\t\n",
    "        sample = np.random.choice(np.arange(len(coocs)), size=batch_size, replace=False)\n",
    "        l_vecs, r_vecs, covals, l_v_bias, r_v_bias = [], [], [], [], []\n",
    "        for chosen in sample:\n",
    "            ind = tuple(coocs[chosen])\n",
    "            l_vecs.append(l_embed[ind[0]])\n",
    "            r_vecs.append(r_embed[ind[1]])\n",
    "            covals.append(comat[ind])\n",
    "            l_v_bias.append(l_biases[ind[0]])\n",
    "            r_v_bias.append(r_biases[ind[1]])\n",
    "        return l_vecs, r_vecs, covals, l_v_bias, r_v_bias\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(num_epochs):\n",
    "        num_batches = int(w_list_size/batch_size)\n",
    "        avg_loss = 0.0\n",
    "        for batch in range(num_batches):\n",
    "            optimizer.zero_grad()\n",
    "            l_vecs, r_vecs, covals, l_v_bias, r_v_bias = gen_batch()\n",
    "            # For pytorch v2 use, .view(-1) in torch.dot here. Otherwise, no need to use .view(-1).\n",
    "            loss = sum([torch.mul((torch.dot(l_vecs[i].view(-1), r_vecs[i].view(-1)) +\n",
    "                    l_v_bias[i] + r_v_bias[i] - np.log(covals[i]))**2,\n",
    "                    wf(covals[i])) for i in range(batch_size)])\n",
    "            avg_loss += loss.data[0]/num_batches\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"\\r Average loss for epoch \"+str(epoch+1)+\": \", avg_loss.item(),end = '')\n",
    "    print(' ')\n",
    "    lem_list =[];rem_list=[];spname_list = []\n",
    "    for word_ind in range(len(vocab)):\n",
    "        # Create embedding by summing left and right embeddings\n",
    "        rem_list.append(l_embed[word_ind].data.numpy())  \n",
    "        spname_list.append(vocab[word_ind])\n",
    "    #cc = pd.DataFrame(np.array(lem_list)[:,:,0])\n",
    "    #cc['Species'] = spname_list\n",
    "    emb_dic = pd.DataFrame(np.array(rem_list)[:,:,0])\n",
    "    #emb_dic['Species'] = spname_list\n",
    "    emb_dic.index = vocab\n",
    "    \n",
    "    return(emb_dic)\n",
    "def make_com_emb(sp_matrix,emb_dic,embed_size):\n",
    "    emb_sent = []\n",
    "    for i in range(sp_matrix.shape[0]):  \n",
    "\n",
    "        pd_table_sub = sp_matrix.iloc[i,:]#>0\n",
    "        c_v = pd_table_sub[pd_table_sub>0].index#np.unique(pd_table_sub.Species.dropna())\n",
    "        init_vec = np.zeros([embed_size])\n",
    "        for ii in c_v:\n",
    "            #i_sum += 1\n",
    "            init_vec += np.float32(emb_dic.loc[ii].values)\n",
    "\n",
    "        emb_sent.append(init_vec/len(c_v)) #/i_sum\n",
    "\n",
    "    #print('\\r  %s.../95104'%i,end = '')\n",
    "    return(np.array(emb_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77d37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc3eb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the recon embeddding module 0804 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor,GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression,ARDRegression\n",
    "\n",
    "import argparse\n",
    "import pprint\n",
    "#import gensim\n",
    "from sklearn.model_selection import cross_validate,LeaveOneOut\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "import pickle\n",
    "import math\n",
    "#import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error,accuracy_score,recall_score\n",
    "import random\n",
    "def to_mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "def to_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "def to_mse(y_true, y_pred):\n",
    "    return (mean_squared_error(y_true, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab63ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the recon embeddding module 0804 \n",
    "\n",
    "#Make loss function\n",
    "def to_mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "def to_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "def to_mse(y_true, y_pred):\n",
    "    return (mean_squared_error(y_true, y_pred))  \n",
    "\n",
    "def emb_names(table):\n",
    "    new_p_list =[]\n",
    "    for i in range(table.shape[1]):\n",
    "        new_p_list.append('em'+str(i))\n",
    "    return(new_p_list)\n",
    "\n",
    "def trans_to_core(spec,core):\n",
    "    new_pd = pd.DataFrame(np.zeros([core.shape[0], spec.shape[1]]))\n",
    "    new_pd.columns = spec.columns\n",
    "    for i_index,i_names in enumerate(new_pd.columns):\n",
    "        try:\n",
    "            #print(rlgh_spec[i_names])\n",
    "            new_pd.iloc[:,i_index] = core[i_names].values\n",
    "        except:\n",
    "            continue\n",
    "    return(new_pd)\n",
    "\n",
    "\n",
    "\n",
    "def gcd_many(s):\n",
    "    g = 0\n",
    "    for i in range(len(s)):\n",
    "        if i == 0:\n",
    "            g = s[i]\n",
    "        else:\n",
    "            g=math.gcd(g,s[i])\n",
    "    return g\n",
    "\n",
    "\n",
    "def to_num(matrix_init):\n",
    "    matrix = matrix_init.copy()\n",
    "    for i in range(matrix.shape[0]):\n",
    "        sub_raw = matrix.iloc[i,:].values\n",
    "        sub_raw_2 = sub_raw[sub_raw>0]\n",
    "        min_raw  = min(sub_raw_2)\n",
    "        sub_raw = sub_raw*10000#sub_raw/min_raw  ## 0/min_raw  = 0\n",
    "        #print(sub_raw[sub_raw>0])\n",
    "        sub_raw = sub_raw/gcd_many(sub_raw[sub_raw>0].astype('int'))\n",
    "    matrix.iloc[i,:] = sub_raw\n",
    "    return(matrix)\n",
    "\n",
    "def pre_glove(Biological_abundance_matrix,environmental_values_matrix,test_method = 'loo',no_components = 256, data_name='Test',\n",
    "              add_tr_env = [], cores = [],core_sp = [],stack = True, glove_abuna = True,random_no = 123,\n",
    "              recon=False,pass_va = False,save_vali = True,cor_emb = False, emb_method = 'plus',test_set = [],num_epochs = 300):\n",
    "    random_no = random_no\n",
    "    if len(test_set) >0:\n",
    "        recon= False\n",
    "        pass_va = False\n",
    "        cores = []\n",
    "        \n",
    "    if emb_method == 'only':\n",
    "        only_emb = True\n",
    "        only_abun = False\n",
    "        print('using the embedding matrix')\n",
    "    elif emb_method == 'none':\n",
    "        only_abun = True\n",
    "        only_emb = False\n",
    "        print('using the abundance matrix')\n",
    "    else:\n",
    "        only_abun = False\n",
    "        only_emb = False\n",
    "        print('using the embedding + abundance matrix')\n",
    "        \n",
    "    random.seed(random_no)\n",
    "    torch.manual_seed(random_no)\n",
    "    torch.cuda.manual_seed(random_no)  # If using CUDA\n",
    "    torch.manual_seed(random_no)\n",
    "\n",
    "    # Set the random seed for NumPy\n",
    "    np.random.seed(0)   \n",
    "        \n",
    "        \n",
    "        \n",
    "    pd_table = Biological_abundance_matrix\n",
    "    envs = environmental_values_matrix\n",
    "    #save_vali = save_vali              ###\n",
    "    if recon ==False:\n",
    "        cores = []\n",
    "    else:\n",
    "        #print(cores.shape)\n",
    "        cor_sp = (set(cores.columns)&set(pd_table.columns))\n",
    "    try:\n",
    "        (envs.shape[1])\n",
    "    except:\n",
    "        envs = pd.DataFrame(envs.T)\n",
    "    \n",
    "    #Statistics of species contained in each sample\n",
    "    print('scale: %s'%pd_table.iloc[0,:].sum())\n",
    "    xmax = 2 \n",
    "    alpha = 0.75\n",
    "    def wf(x):\n",
    "        return x\n",
    "    if glove_abuna == True:\n",
    "        if (pd_table.iloc[0,:].sum() > 1.3):\n",
    "            pd_table_2 = pd_table\n",
    "            #if np.min(pd_table_2[pd_table_2>0]).min()>1:\n",
    "            def wf(x):\n",
    "                if x < xmax:\n",
    "                    return (x/xmax)**alpha\n",
    "                return 1\n",
    "        else:\n",
    "            pd_table_2 = to_num(pd_table)\n",
    "\n",
    "    #pd_table_sub = pd_table.iloc[i,:]*250\n",
    "    if recon == True:\n",
    "        if cor_emb == True:\n",
    "            pd_table_2 = pd_table_2[cor_sp]\n",
    "    if only_abun == False:\n",
    "        sent = make_sents(pd_table)\n",
    "        print('creating the embedding now...')   ###\n",
    "    \n",
    "        embed_size = no_components\n",
    "        vocab = pd_table.columns\n",
    "        #embed_size = 256\n",
    "        emb_dic = glove_dic(sent,embed_size,vocab,wf,glove_abuna = glove_abuna,num_epochs = num_epochs)\n",
    "        \n",
    "        paragraph_list = make_com_emb(pd_table,emb_dic,embed_size)\n",
    "        emb_name_list = list(emb_names(paragraph_list))\n",
    "\n",
    "        #print('Species num(availble in asscoiation): %s' % len(corpus_model.dictionary))\n",
    "    print('Species num: %s' % pd_table.shape[1])\n",
    "    print('Samples num: %s' % pd_table.shape[0])\n",
    "\n",
    "    #Glove model training\n",
    "\n",
    "    #print(np.sum(np.array(paragraph_list)))\n",
    "        \n",
    "    print(' ')\n",
    "    print('=================================')\n",
    "    print('training the embedding ......')\n",
    "    \n",
    "    # Three machine learning methods\n",
    "    #rfr = RandomForestRegressor(n_estimators = 1000,n_jobs=-2)#n_estimators = 1000, min_sample_leaf10)\n",
    "    #etr = ExtraTreesRegressor(n_estimators = 1000,n_jobs=-2)\n",
    "    #gbm = LGBMRegressor(objective='regression', num_leaves=31, learning_rate=0.05, n_estimators=1000,n_jobs=-2)\n",
    "\n",
    "    \n",
    "    if only_emb == True:  \n",
    "        para_names =  emb_name_list   \n",
    "\n",
    "    elif only_abun == True:\n",
    "        para_names = pd_table.columns\n",
    "        paragraph_list = np.array(pd_table)\n",
    "    else:\n",
    "        print('adding the speiese abunadance now...')\n",
    "        paragraph_list = np.concatenate((np.array(paragraph_list),np.array(pd_table)), axis = 1)    ###\n",
    "        data_name = '%s_rich'%data_name\n",
    "        para_names =  emb_name_list + list(pd_table.columns)\n",
    "        #print([len(para_names),len(list(emb_names(np.array(paragraph_list)))),len(list(emb_names(np.array(paragraph_list))))])    \n",
    "        #print(paragraph_list.shape)\n",
    "        \n",
    "    if  len(add_tr_env) !=0:\n",
    "        data_name = '%s_added'%data_name\n",
    "        paragraph_list = np.concatenate((np.array(paragraph_list),np.array(add_tr_env)), axis = 1)\n",
    "        para_names = emb_name_list + list(add_tr_env.columns)\n",
    "        #print(paragraph_list.shape)\n",
    "    #print(len(para_names))\n",
    "\n",
    "        #print(paragraph_list.shape,cores.shape)\n",
    "    if pass_va == False :  \n",
    "        if test_method == 'cv':\n",
    "            cs=train_data_pre(paragraph_list,envs,data_name=data_name,test_method='cv',para_names = para_names, stack = stack, save_vali = save_vali)\n",
    "        elif test_method == 'loo':\n",
    "            cs=train_data_pre(paragraph_list,envs,data_name=data_name,test_method='loo',para_names = para_names, stack = stack, save_vali = save_vali)\n",
    "        elif test_method == 'diy':\n",
    "            cs=train_data_pre(paragraph_list,envs,data_name=data_name,test_method='diy',para_names = para_names, stack = stack,test_set=test_set)\n",
    "\n",
    "        else:\n",
    "            print('you shall choose one of these validate method:  cv, loo,diy') \n",
    "    if len(cores) !=0:\n",
    "        print('reconstructing the %s'%data_name)\n",
    "        cores_index = cores.index\n",
    "        if len(cores.columns) > len(pd_table.columns):      \n",
    "            cores = cores[cor_sp]\n",
    "            \n",
    "        elif len(cores.columns) < len(pd_table.columns):\n",
    "            cores = trans_to_core(pd_table,cores)\n",
    "        paragraph_list2 = []\n",
    "        #Generate embedding for each sample\n",
    "        cores.columns = pd_table.columns\n",
    "        if only_abun == False:\n",
    "            paragraph_list2 = make_com_emb(cores,emb_dic,embed_size)\n",
    "            if only_emb == False:\n",
    "                paragraph_list = np.concatenate((np.array(paragraph_list)[:,:no_components],np.array(pd_table)), axis = 1)\n",
    "                para_names = emb_name_list+list(pd_table.columns)\n",
    "                cores = np.concatenate((np.array(paragraph_list2),np.array(cores)), axis = 1)    #\n",
    "            else:\n",
    "                cores = np.array(paragraph_list2)\n",
    "        cs = recon_data(paragraph_list,envs,cores,cores_index,pass_va,data_name=data_name,trastacks = stack,test_method=test_method,para_names = para_names)\n",
    "        \n",
    "    print('The operation is over. Have a nice day!')\n",
    "    print('')\n",
    "    print('')\n",
    "\n",
    "    return(cs)\n",
    "\n",
    "def train_data_pre(X,y,data_name,GBR = False,n_estimators = 1000,test_method = 'loo',stack = True,save_vali = False,para_names=[],test_set = []):\n",
    "    if save_vali == True:\n",
    "        op_pirnt = 'on'\n",
    "    else:\n",
    "        op_pirnt = 'off'\n",
    "    print('save validation results option: %s'%op_pirnt)\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    acc_tt_list = []\n",
    "    try:\n",
    "        y.shape[1]\n",
    "        init_name = data_name\n",
    "        print('has multi environmental values')\n",
    "        for y_col in range(y.shape[1]):\n",
    "            pd_acc = pd.DataFrame(np.zeros([y.shape[1]*2,5]))\n",
    "            data_name = '%s_%s'%(init_name,y.columns[y_col])\n",
    "            X =np.array(X)\n",
    "            yy =np.array(y.iloc[:,y_col])\n",
    "            xx= X[~pd.isnull(yy)]\n",
    "            print('sub mission:  training %s now...'%data_name)\n",
    "            if test_method== 'loo':\n",
    "                acc = train_data_loo(X=xx,y = yy,GBR = False,data_name=data_name,n_estimators = 1000,stack = stack, save_vali = save_vali,test_method=test_method,para_names = para_names)\n",
    "            elif test_method== 'cv':\n",
    "                acc = train_data_cv(X=xx,y = yy,GBR = False,data_name=data_name,n_estimators = 1000,stack = stack, save_vali = save_vali,test_method=test_method,para_names = para_names)\n",
    "            elif test_method== 'diy':\n",
    "                return(train_data_diy(X=xx,y = yy,test_set = test_set, GBR = False,data_name=data_name,n_estimators = 1000,stack = stack, test_method=test_method,para_names = para_names))\n",
    "\n",
    "        acc_tt_list.append([data_name,*acc])   ###########\n",
    "        return(acc_tt_list)\n",
    "    except:\n",
    "        if test_method == 'split':\n",
    "            return(train_data_split_across(X,y,GBR = False,data_name=data_name,n_estimators = 1000,stack = stack,test_method=test_method,para_names = para_names))\n",
    "        if test_method == 'loo':\n",
    "            return(train_data_loo(X,y,GBR = False,data_name = data_name,n_estimators = 1000,stack = stack, save_vali = save_vali,test_method=test_method,para_names = para_names))\n",
    "        elif test_method == 'cv':\n",
    "            return(train_data_cv(X,y,GBR = False,data_name=data_name,n_estimators = 1000,stack = stack, save_vali = save_vali,test_method=test_method,para_names = para_names))     \n",
    "        elif test_method== 'diy':\n",
    "            return(train_data_diy(X=xx,y = yy,test_set = test_set, GBR = False,data_name=data_name,n_estimators = 1000,stack = stack, test_method=test_method,para_names = para_names)\n",
    ")\n",
    "def train_data_cv(X,y,data_name,test_method = [],para_names = [],GBR = False,n_estimators = 1000,stack = True,save_vali = False):       \n",
    "    \n",
    "    base_list = np.array(range(X.shape[0]))\n",
    "    x_train_names = para_names\n",
    "    ran_index = np.random.permutation(base_list)\n",
    "    y= np.array(y)\n",
    "    X = np.array(X)\n",
    "    X = X[ran_index]\n",
    "    y =y[ran_index]\n",
    "    y_token = 'multi'\n",
    "    y_test_l = []\n",
    "    pre_l = []\n",
    "    kf = KFold(n_splits=5)\n",
    "\n",
    "    for train, test in kf.split(base_list):\n",
    "    \n",
    "        try:\n",
    "            x_train, x_test, y_train, y_test = X[train],X[test],y[train],y[test]\n",
    "        except:\n",
    "            x_train, x_test, y_train, y_test = X.loc(axis=0)[train],X.loc(axis=0)[test],y.loc(axis=0)[train],y.loc(axis=0)[test]        \n",
    "        #print(x_train.shape,y_train.shape)\n",
    "        predict3 = make_predict(x_train, x_test, y_train, y_test,test_method,data_name,x_train_names)\n",
    "        if stack == True:\n",
    "\n",
    "            predict3 = to_stack_2(x_train,y_train,predict3)\n",
    "\n",
    "        y_test_l.append(y_test.reshape(-1,1))\n",
    "        pre_l.append(predict3)\n",
    "    \n",
    "    pd_re = tail(pre_l,y_test_l,predict3,data_name,stack = stack,test_method ='cv',save_vali = save_vali)\n",
    "    return(pd_re) #[choose_predict(acc)] #,y_test, predict3,rfr\n",
    "\n",
    "\n",
    "\n",
    "def train_data_diy(X,y,test_set,data_name,test_method = [],para_names = [],GBR = False,n_estimators = 1000,stack = True):       \n",
    "\n",
    "    x_train_names = para_names\n",
    "    X =np.array(X)\n",
    "    y = np.array(y)\n",
    "    y_token = 'multi'\n",
    "    print('test',X.shape)\n",
    "\n",
    "    \n",
    "    x_train = X\n",
    "    x_test = y\n",
    "    y_test = []\n",
    "    y_train = test_set\n",
    "    #print('MEASURED     RECON       RICHNESS')\n",
    "\n",
    "    predict3 = make_predict(x_train, x_test, y_train, y_test,test_method,data_name,x_train_names,print_importa = False)\n",
    "    if stack == True:\n",
    "        #print(y_train.shape,x_train.shape,np.array(predict3).shape)\n",
    "\n",
    "        predict3 = to_stack_2(x_train,y_train,predict3)\n",
    "        #print('OK')\n",
    "    return(predict3) #[choose_predict(acc)] #,y_test, predict3,rfr\n",
    "\n",
    "def make_predict(x_train, x_test, y_train, y_test,data_name , test_method,x_train_names,n_estimators = 1000, GBR = False, print_importa = True):\n",
    "    \n",
    "    if len(x_train.shape) == 1:\n",
    "            x_train = x_train.reshape(-1, 1)\n",
    "            x_test = x_test.reshape(-1, 1)\n",
    "    if len(y_test)>0:\n",
    "        if len(y_test.shape) == 1:\n",
    "            y_train = y_train.reshape(-1, 1)\n",
    "            y_test = y_test.reshape(-1, 1)\n",
    "            y_token = 'single'\n",
    "    #print([x_train.shape,x_test.shape])\n",
    "    predict3 = three_models( x_train,y_train,x_test,test_method,data_name, x_train_names, print_importa = True,print_model = False)\n",
    "\n",
    "\n",
    "    return(predict3)\n",
    "        \n",
    "def to_stack(x_train,y_train,predict3):\n",
    "    rfr2,best_no, quit_lightGBM = mulit_model_stack2(x_train,y_train)\n",
    "    #print(rfr2,best_no, quit_lightGBM)\n",
    "    acc3_l = []\n",
    "    acc3_l.append(np.array(predict3)[0,:])\n",
    "    acc3_l.append(np.array(predict3)[1,:])\n",
    "    acc3_l.append(np.array(predict3)[2,:])\n",
    "    #print(acc3_l)\n",
    "    #print(predict3)\n",
    "    if quit_lightGBM == True:\n",
    "        predict3.append(rfr2.predict((np.array(acc3_l).T)[:,:1]).reshape(-1,1)\n",
    "                       )\n",
    "    else:\n",
    "        predict3.append(rfr2.predict(np.array(acc3_l).T).reshape(-1,1))\n",
    "    #print(predict3)\n",
    "    return(predict3)\n",
    "\n",
    "def to_stack_2(x_train,y_train,predict3):\n",
    "    #print(x_train.shape,y_train.shape)\n",
    "    rfr2,best_no, quit_lightGBM = mulit_model_stack2(x_train,y_train,test_method = 'cv')\n",
    "    #print(rfr2,best_no, quit_lightGBM)\n",
    "    predict3 = np.array(predict3).T   # (3,8)\n",
    "    acc3_l = []\n",
    "    acc3_l.append(predict3[:,0])\n",
    "    acc3_l.append(predict3[:,1])\n",
    "    acc3_l.append(predict3[:,2])\n",
    "    #print(acc3_l)\n",
    "    #print(predict3)\n",
    "    if quit_lightGBM == True:\n",
    "                predict3 = np.concatenate((predict3,rfr2.predict(np.array(acc3_l).T)[:,:1].reshape(-1,1)\n",
    "                                          ), axis = 1) \n",
    "    else:\n",
    "        predict3 = np.concatenate((predict3,rfr2.predict(np.array(acc3_l).T).reshape(-1,1)), axis = 1) \n",
    "    #print(predict3)\n",
    "    return(predict3)\n",
    "    \n",
    "    \n",
    "def train_data_loo(X,y,test_method, data_name,para_names,GBR = False,n_estimators = 1000, stack = False,save_vali = False):\n",
    "    y = np.array(y)\n",
    "    x_train_names = para_names\n",
    "    X =np.array(X)\n",
    "    y_token = 'multi'\n",
    "    X_no =range(X.shape[0])\n",
    "    loo = LeaveOneOut()\n",
    "    y_test_l = []\n",
    "    pre_l = []\n",
    "    print('MEASURED     RECON       RICHNESS')\n",
    "\n",
    "    # Loo cross_validate\n",
    "    for train, test in loo.split(X_no):\n",
    "\n",
    "        x_train, x_test, y_train, y_test = X[train],X[test],y[train],y[test]\n",
    "\n",
    "        #x_train, x_test, y_train, y_test = train_test_split( X,y, test_size=0.50, random_state=33)\n",
    "        \n",
    "        predict3 = make_predict(x_train, x_test, y_train, y_test,test_method,data_name,x_train_names)\n",
    "\n",
    "        if stack == True:\n",
    "            predict3 = to_stack(x_train,y_train,predict3)\n",
    "\n",
    "        y_test_l.append(y_test)\n",
    "        pre_l.append(predict3)\n",
    "        print(\"   %.3f      %.3f         %s\" %(pre_l[-1][0],y_test_l[-1][0],len(x_test[x_test>0])))\n",
    "    #acc = []\n",
    "    pd_re = tail(pre_l,y_test_l,predict3,data_name,stack = stack,test_method = 'loo',save_vali = save_vali)\n",
    "    \n",
    "    return(pd_re) #[choose_predict(acc)] #,y_test, predict3,rfr\n",
    "\n",
    "def tail(pre_l,y_test_l,predict3,data_name,stack = False,test_method = 'loo',save_vali = False):\n",
    "    if test_method == 'cv':\n",
    "        predict3 = np.vstack(pre_l)\n",
    "        #print(predict3.shape)\n",
    "    else:\n",
    "        predict3 = np.array(pre_l)\n",
    "    y_test =  np.vstack(y_test_l).reshape(-1, 1)\n",
    "\n",
    "    acc_r2 = []\n",
    "    acc_rmse = []\n",
    "    #print(y_test.shape,predict3.shape)\n",
    "    #if test_method == 'cv':\n",
    "    #predict3 = predict3.T\n",
    "    for i in range(predict3.shape[1]):\n",
    "\n",
    "        try:\n",
    "            acc_r2.append(r2_score(y_test, predict3[:,i].reshape(-1, 1)))\n",
    "            acc_rmse.append(to_rmse(y_test, predict3[:,i].reshape(-1, 1)))\n",
    "        except:\n",
    "            acc_r2.append(r2_score(y_test, predict3[:,i]))\n",
    "            acc_rmse.append(to_rmse(y_test, predict3[:,i]))\n",
    "    try:\n",
    "        acc_r2.append(r2_score(y_test, predict3.mean(1).reshape(-1, 1)))\n",
    "        acc_rmse.append(to_rmse(y_test, predict3.mean(1).reshape(-1, 1)))\n",
    "    except:\n",
    "        acc_r2.append(r2_score(y_test, predict3[:,i].mean(1)))\n",
    "        acc_rmse.append(to_rmse(y_test, predict3[:,i].mean(1)))\n",
    "    acc = [acc_r2,acc_rmse]\n",
    "      \n",
    "    print(\"___________result___________\")\n",
    "    print(\"r2_score\")\n",
    "    if stack == False:\n",
    "        print(\"   rfr       etr          lightGBM\")\n",
    "        print(\"   %.3f     %.3f        %.3f\" %(acc[0][0],acc[0][1],acc[0][2]))\n",
    "        print(\"RMSEP\")\n",
    "\n",
    "        print(\"   rfr       etr          lightGBM\")\n",
    "        print(\"   %.3f     %.3f        %.3f\" %(acc[1][0],acc[1][1],acc[1][2]))\n",
    "        va_cloumns = [\"rfr\"  ,     \"etr\",          \"lightGBM\",  \"true\"]\n",
    "    else:\n",
    "\n",
    "        print(\"   rfr       etr       lightGBM       stack       mean\")\n",
    "        print(\"   %.3f     %.3f        %.3f       %.3f       %.3f\" %(acc[0][0],acc[0][1],acc[0][2],acc[0][3],acc[0][4]))\n",
    "        print(\"RMSEP\")\n",
    "\n",
    "        print(\"   rfr       etr       lightGBM       stack       mean\")\n",
    "        print(\"   %.3f     %.3f        %.3f       %.3f       %.3f\" %(acc[1][0],acc[1][1],acc[1][2],acc[1][3],acc[1][4]))\n",
    "        va_cloumns = [\"rfr\"  ,     \"etr\",          \"lightGBM\",   \"stack\",'mean',\"true\"]\n",
    "    ##pickle.dump([predict3,y_test],open('./pkl/cross_val_%s.pkl'%(data_name),\"wb\"), protocol=3)\n",
    "    #print(predict3.shape,y_test.shape)\n",
    "\n",
    "\n",
    "    if save_vali == True:\n",
    "        try:\n",
    "            va_results = pd.DataFrame(np.concatenate((predict3[:,:],y_test),axis = 1))\n",
    "        except:\n",
    "            va_results = pd.DataFrame(np.concatenate((predict3[:,:,0],y_test),axis = 1))\n",
    "        va_results.columns = va_cloumns[:-1]\n",
    "        va_results.to_csv('vadata_%s_%s.csv'%(test_method,data_name))\n",
    "    #pickle.dump([predict3,y_test],open('./pkl/vali_%s_%s.pkl'%(test_method,data_name),\"wb\"), protocol=3)\n",
    "    #r2_score(y_test.reshape((-1, 1)), predict3[-1].reshape((-1, 1)))\n",
    "    #mo.score(y_test.reshape((-1, 1)), predict3[0]\n",
    "    model_num = predict3.shape[1]\n",
    "    pd_re = pd.DataFrame(acc)\n",
    "    pd_re.columns =  ['rfr','etr','lightGBM','stack','mean']#[:model_num]\n",
    "    pd_re.index = ['r2_score','rmsep']\n",
    "    pd_re.to_csv('varesult_%s_%s.csv'%(test_method,data_name))   ######\n",
    "    return(pd_re)\n",
    "\n",
    "\n",
    "\n",
    "def recon_data(paragraph_list,envs,cores,cores_index,pass_va,para_names,test_method, data_name,GBR =False,trastacks = True):\n",
    "    ## starting recon data process!\n",
    "    y_train = envs\n",
    "    x_train_names = para_names\n",
    "    x_train = np.array(paragraph_list)\n",
    "    x_test = cores\n",
    "    predict3 = []\n",
    "    \n",
    "    if len(y_train.shape) == 1:\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "        y_token = 'single'\n",
    "    if pass_va == False:\n",
    "        print_importa = False\n",
    "    else:\n",
    "        print_importa = True\n",
    "    predict3 = three_models( x_train,y_train,x_test,test_method,data_name,x_train_names, print_importa = print_importa,print_model = False)\n",
    "                 \n",
    "    ## starting stack!!\n",
    "    if trastacks == True:\n",
    "        model_num = 4\n",
    "        rfr2,best_no, quit_lightGBM = mulit_model_stack2(x_train,y_train)\n",
    "\n",
    "        acc3_l = []\n",
    "        acc3_l.append(np.array(predict3)[0,:])\n",
    "        acc3_l.append(np.array(predict3)[1,:])\n",
    "        acc3_l.append(np.array(predict3)[2,:])        #print(np.array(acc3_l).shape)\n",
    "        if quit_lightGBM == True:\n",
    "            predict3.append(rfr2.predict((np.array(acc3_l).T)[:,:1]).reshape(-1,1))\n",
    "        else:\n",
    "            predict3.append(rfr2.predict(np.array(acc3_l).T).reshape(-1,1)\n",
    "                           )\n",
    "    else:\n",
    "        model_num = 3\n",
    "        print('without stack process...')\n",
    "    model_name_list = []\n",
    "    for i in range(model_num):\n",
    "        if i == 3:\n",
    "            i = -1\n",
    "        model_name = ['rfr','etr','lightGBM','stack'][i]\n",
    "        pd.DataFrame(predict3).T\n",
    "        #pd.DataFrame(predict3[i]).to_csv('./recon/%s_%s.csv'%(data_name,model_name))\n",
    "        print('created the %s_%s model output'%(data_name,model_name))\n",
    "        model_name_list.append(model_name)\n",
    "    ## reconstructions has been done\n",
    "    if data_name != 'test':\n",
    "        t_pd = pd.DataFrame(predict3).T\n",
    "        t_pd.columns = model_name_list\n",
    "        if model_name_list[-1] =='stack':\n",
    "            #t_pd['stack'] =  t_pd['stack'].apply(lambda x: x.replace('[','').replace(']','')) \n",
    "            t_pd['stack'] =  t_pd['stack'].apply(lambda x: x[0]) \n",
    "            t_pd['stack'] = t_pd['stack'].astype(float)\n",
    "        t_pd.index = cores_index\n",
    "        t_pd.to_csv('%s_core.csv'%(data_name))\n",
    "    return(t_pd)\n",
    "\n",
    "\n",
    "\n",
    "def mulit_model_stack2(x_train,y_train,GBR = False,test_method = 'loo'):\n",
    "    \n",
    "    test4 = []\n",
    "    predict4 = []\n",
    "    predict5_l = []\n",
    "    best_no = []\n",
    "    acc_list = []\n",
    "    acc_list2 = []\n",
    "\n",
    "    x_train=np.array(x_train)  #\n",
    "    y_train = np.array(y_train)\n",
    "    kf = KFold(n_splits=5)\n",
    "\n",
    "    \n",
    "    base_list = np.array(range(y_train.shape[0]))\n",
    "    ran_index = np.random.permutation(base_list)\n",
    "    x_train = x_train[ran_index]\n",
    "    y_train =y_train[ran_index]\n",
    "    pre_l = []\n",
    "    for train, test in kf.split(ran_index):\n",
    "        predict5 =[]\n",
    "        \n",
    "        try:\n",
    "            x_train2, x_test2, y_train2, y_test2 = x_train[train],x_train[test],y_train[train],y_train[test]\n",
    "        except:\n",
    "            x_train2, x_test2, y_train2, y_test2 = x_train.loc(axis=0)[train],x_train.loc(axis=0)[test],y_train.loc(axis=0)[train],y_train.loc(axis=0)[test]\n",
    "            \n",
    "        predict3 = three_models( x_train2,y_train2,x_test2, print_importa = False,print_model = False)\n",
    "        #print('pred',np.array(predict3).shape)\n",
    "        test4.append(y_test2.reshape(-1,1))\n",
    "        predict4.append(np.array(predict3).T)\n",
    "        # predict3 (3, 4)\n",
    "        use_inf = True\n",
    "        if test_method == 'loo':\n",
    "                predict5.append(np.array(predict3)[0,:])\n",
    "                predict5.append(np.array(predict3)[1,:])\n",
    "                predict5.append(np.array(predict3)[2,:])        \n",
    "        else:\n",
    "            if use_inf == True:\n",
    "                predict5.append(np.array(predict3)[0,:])\n",
    "                predict5.append(np.array(predict3)[1,:])\n",
    "                predict5.append(np.array(predict3)[2,:])\n",
    "            else:\n",
    "                #print(len( predict5),np.array(predict3)[:,2])\n",
    "                print(xxxxxxxxxx)\n",
    "                #print(np.array(predict3).shape)\n",
    "                predict5.append((np.array(predict3)[0,:]+np.array(predict3)[1,:]).T/2)\n",
    "                predict5.append((np.array(predict3)[0,:]+np.array(predict3)[2,:]).T/2)\n",
    "                predict5.append((np.array(predict3)[1,:]+np.array(predict3)[2,:]).T/2)\n",
    "                predict5.append((np.array(predict3)[1,:]+np.array(predict3)[2,:]+np.array(predict3)[0,:]).T/3)\n",
    "            #print(len( predict5),np.array(predict3)[:,2])\n",
    "        predict5_l.append(np.array(predict5).T)\n",
    "        #print('add')\n",
    "        #print(np.array(predict3).shape,lr.predict(x_test2).shape)\n",
    "        #print(x_test2.shape,lr.predict(x_test2).shape, y_test2.reshape(-1,1).shape,np.array(predict5).shape)\n",
    "        acc2 = []\n",
    "        predict5= np.array(predict5).T\n",
    "        #print(len(predict5_l),predict5.shape,len(test4))\n",
    "        #print(predict5.shape,y_test2.shape,predict5[0,:].shape)\n",
    "        \n",
    "    rfr2 = LinearRegression() #ARDRegression() LinearRegression() \n",
    "    #n_estimators = 1000, min_sample_leaf10)\n",
    "    predict5_lt = np.vstack(predict5_l)\n",
    "    #print('pred2',predict5_lt.shape)\n",
    "\n",
    "    test_lt = np.vstack(test4)\n",
    "\n",
    "\n",
    "    #if np.mean(predict5_lt[:,-2]) <0:\n",
    "    #    predict5_lt = predict5_lt[:,:1]\n",
    "    #    quit_lightGBM = True\n",
    "    #else:\n",
    "    quit_lightGBM = False\n",
    "    #print('pred2',predict5_lt.shape)\n",
    "    #print(quit_lightGBM)\n",
    "    rfr2.fit(np.vstack(predict5_lt),test_lt) \n",
    "    return(rfr2,best_no,quit_lightGBM)\n",
    "\n",
    "def three_models(x_train,y_train,x_test,test_method =[],data_name=[],x_train_names=[],GBR =False, print_importa = True,print_model = False,n_estimators = 1000,random_no = 123):\n",
    "    from numpy import random\n",
    "    #print(x_train.shape,'three_models')\n",
    "    predict3 = []\n",
    "    rfr = RandomForestRegressor(n_estimators = 1000,n_jobs=-2, random_state = random.seed(random_no))#n_estimators , min_sample_leaf10)\n",
    "    # training\n",
    "    rfr.fit(x_train,y_train) #.reshape((-1, 1)),\n",
    "    # predict and save forecast result\n",
    "    predict3.append(rfr.predict(x_test)) # .reshape((-1, 1))\n",
    "\n",
    "    # etr regression\n",
    "    etr = ExtraTreesRegressor(n_estimators = n_estimators,n_jobs=-2, random_state = random.seed(random_no))\n",
    "    etr.fit(x_train,y_train) #.reshape((-1, 1)),\n",
    "    predict3.append(etr.predict(x_test))\n",
    "    if GBR == True:\n",
    "        gbr = GradientBoostingRegressor(n_estimators = n_estimators,n_jobs=-2, random_state = random.seed(random_no))\n",
    "        gbr.fit(x_train,y_train) #.reshape((-1, 1)),\n",
    "        gbr_y_predict = gbr.predict(x_test)\n",
    "        predict3.append(gbr.predict(x_test))\n",
    "        predict3 = np.array(predict3)\n",
    "        print('the 3rd model is GBR')\n",
    "        #imp_table = pd.DataFrame([rfr.feature_importances_,etr.feature_importances_,gbr.feature_importances_]).T\n",
    "        imp_table.columns = ['rfr','etr']\n",
    "    else:\n",
    "        gbm = LGBMRegressor(objective='regression', num_leaves=31, learning_rate=0.05, n_estimators=1000,n_jobs=-2,random_state = random.seed(random_no),verbose = -1)\n",
    "\n",
    "        gbm.fit(x_train,y_train) #.reshape((-1, 1)),\n",
    "        predict3.append(gbm.predict(x_test))\n",
    "        imp_table = pd.DataFrame([rfr.feature_importances_,etr.feature_importances_,gbm.feature_importances_]).T\n",
    "        imp_table.columns = ['rfr','etr','LightGBM']\n",
    "    if print_importa == True:\n",
    "        \n",
    "\n",
    "        if  len(x_train_names) >0:\n",
    "            imp_table.index = x_train_names\n",
    "        imp_table = imp_table.sort_values(['rfr'],ascending = False)\n",
    "        imp_table.to_csv('importances_%s_%s.csv'%(test_method,data_name))\n",
    "    if print_model == False:\n",
    "        return(predict3)\n",
    "    else:\n",
    "        return(predict3,rfr,etr,model3)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def to_pers(df):\n",
    "    row_sums = df.sum(axis=1)\n",
    "    # Divide each value in the row by the corresponding row sum and multiply by 100\n",
    "    df_percentage = df.div(row_sums, axis=0) \n",
    "    return(df_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0f05063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the embedding + abundance matrix\n",
      "scale: 0.8427\n",
      "  167...creating the embedding now...\n",
      " Average loss for epoch 1000:  7.211855607312498 \n",
      "Species num: 277\n",
      "Samples num: 167\n",
      " \n",
      "=================================\n",
      "training the embedding ......\n",
      "adding the speiese abunadance now...\n",
      "save validation results option: on\n",
      "has multi environmental values\n",
      "sub mission:  training id_rlgh3_ae256_1707_addnum_x now...\n",
      "___________result___________\n",
      "r2_score\n",
      "   rfr       etr       lightGBM       stack       mean\n",
      "   0.820     0.836        0.855       0.859       0.855\n",
      "RMSEP\n",
      "   rfr       etr       lightGBM       stack       mean\n",
      "   0.326     0.312        0.293       0.289       0.293\n",
      "reconstructing the id_rlgh3_ae256_1707_addnum\n",
      "created the id_rlgh3_ae256_1707_addnum_rfr model output\n",
      "created the id_rlgh3_ae256_1707_addnum_etr model output\n",
      "created the id_rlgh3_ae256_1707_addnum_lightGBM model output\n",
      "created the id_rlgh3_ae256_1707_addnum_stack model output\n",
      "The operation is over. Have a nice day!\n",
      "\n",
      "\n",
      "using the embedding matrix\n",
      "scale: 0.8427\n",
      "  167...creating the embedding now...\n",
      " Average loss for epoch 1000:  7.211855607312498 \n",
      "Species num: 277\n",
      "Samples num: 167\n",
      " \n",
      "=================================\n",
      "training the embedding ......\n",
      "save validation results option: on\n",
      "has multi environmental values\n",
      "sub mission:  training id_rlgh3_e256_1707_x now...\n",
      "___________result___________\n",
      "r2_score\n",
      "   rfr       etr       lightGBM       stack       mean\n",
      "   0.740     0.736        0.747       0.752       0.753\n",
      "RMSEP\n",
      "   rfr       etr       lightGBM       stack       mean\n",
      "   0.392     0.396        0.387       0.383       0.382\n",
      "reconstructing the id_rlgh3_e256_1707\n",
      "created the id_rlgh3_e256_1707_rfr model output\n",
      "created the id_rlgh3_e256_1707_etr model output\n",
      "created the id_rlgh3_e256_1707_lightGBM model output\n",
      "created the id_rlgh3_e256_1707_stack model output\n",
      "The operation is over. Have a nice day!\n",
      "\n",
      "\n",
      "using the abundance matrix\n",
      "scale: 0.8427\n",
      "Species num: 277\n",
      "Samples num: 167\n",
      " \n",
      "=================================\n",
      "training the embedding ......\n",
      "save validation results option: on\n",
      "has multi environmental values\n",
      "sub mission:  training id_rlgh3_a256_1707_x now...\n",
      "___________result___________\n",
      "r2_score\n",
      "   rfr       etr       lightGBM       stack       mean\n",
      "   0.800     0.847        0.826       0.859       0.851\n",
      "RMSEP\n",
      "   rfr       etr       lightGBM       stack       mean\n",
      "   0.344     0.301        0.321       0.289       0.297\n",
      "reconstructing the id_rlgh3_a256_1707\n",
      "created the id_rlgh3_a256_1707_rfr model output\n",
      "created the id_rlgh3_a256_1707_etr model output\n",
      "created the id_rlgh3_a256_1707_lightGBM model output\n",
      "created the id_rlgh3_a256_1707_stack model output\n",
      "The operation is over. Have a nice day!\n",
      "\n",
      "\n",
      "using the embedding + abundance matrix\n",
      "scale: 0.8427\n",
      "  167...creating the embedding now...\n",
      " Average loss for epoch 1000:  7.211855607312498 \n",
      "Species num: 277\n",
      "Samples num: 167\n",
      " \n",
      "=================================\n",
      "training the embedding ......\n",
      "adding the speiese abunadance now...\n",
      "reconstructing the id_rlgh_ae256_1707_addnum\n",
      "created the id_rlgh_ae256_1707_addnum_rfr model output\n",
      "created the id_rlgh_ae256_1707_addnum_etr model output\n",
      "created the id_rlgh_ae256_1707_addnum_lightGBM model output\n",
      "created the id_rlgh_ae256_1707_addnum_stack model output\n",
      "The operation is over. Have a nice day!\n",
      "\n",
      "\n",
      "using the embedding matrix\n",
      "scale: 0.8427\n",
      "  167...creating the embedding now...\n",
      " Average loss for epoch 1000:  7.211855607312498 \n",
      "Species num: 277\n",
      "Samples num: 167\n",
      " \n",
      "=================================\n",
      "training the embedding ......\n",
      "reconstructing the id_rlgh_e256_1707\n",
      "created the id_rlgh_e256_1707_rfr model output\n",
      "created the id_rlgh_e256_1707_etr model output\n",
      "created the id_rlgh_e256_1707_lightGBM model output\n",
      "created the id_rlgh_e256_1707_stack model output\n",
      "The operation is over. Have a nice day!\n",
      "\n",
      "\n",
      "using the abundance matrix\n",
      "scale: 0.8427\n",
      "Species num: 277\n",
      "Samples num: 167\n",
      " \n",
      "=================================\n",
      "training the embedding ......\n",
      "reconstructing the id_rlgh_a256_1707\n",
      "created the id_rlgh_a256_1707_rfr model output\n",
      "created the id_rlgh_a256_1707_etr model output\n",
      "created the id_rlgh_a256_1707_lightGBM model output\n",
      "created the id_rlgh_a256_1707_stack model output\n",
      "The operation is over. Have a nice day!\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfr</th>\n",
       "      <th>etr</th>\n",
       "      <th>lightGBM</th>\n",
       "      <th>stack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>4.897571</td>\n",
       "      <td>4.926087</td>\n",
       "      <td>4.9499</td>\n",
       "      <td>4.880102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>4.812274</td>\n",
       "      <td>4.833755</td>\n",
       "      <td>4.743672</td>\n",
       "      <td>4.751666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.3</th>\n",
       "      <td>4.978854</td>\n",
       "      <td>4.951225</td>\n",
       "      <td>4.823021</td>\n",
       "      <td>4.860961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.8</th>\n",
       "      <td>4.940128</td>\n",
       "      <td>4.878684</td>\n",
       "      <td>4.7317</td>\n",
       "      <td>4.768909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.3</th>\n",
       "      <td>4.748331</td>\n",
       "      <td>4.741096</td>\n",
       "      <td>4.716704</td>\n",
       "      <td>4.657279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.8</th>\n",
       "      <td>4.974619</td>\n",
       "      <td>4.939024</td>\n",
       "      <td>4.701327</td>\n",
       "      <td>4.821513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.3</th>\n",
       "      <td>4.752634</td>\n",
       "      <td>4.768663</td>\n",
       "      <td>4.727463</td>\n",
       "      <td>4.689459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.8</th>\n",
       "      <td>4.801476</td>\n",
       "      <td>4.805669</td>\n",
       "      <td>4.717717</td>\n",
       "      <td>4.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.3</th>\n",
       "      <td>4.797783</td>\n",
       "      <td>4.818087</td>\n",
       "      <td>4.726396</td>\n",
       "      <td>4.733779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.8</th>\n",
       "      <td>5.012914</td>\n",
       "      <td>4.979691</td>\n",
       "      <td>4.82601</td>\n",
       "      <td>4.885367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.8</th>\n",
       "      <td>5.085126</td>\n",
       "      <td>4.950391</td>\n",
       "      <td>4.92746</td>\n",
       "      <td>4.857789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.8</th>\n",
       "      <td>4.838069</td>\n",
       "      <td>4.91594</td>\n",
       "      <td>4.716451</td>\n",
       "      <td>4.831464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.8</th>\n",
       "      <td>5.081228</td>\n",
       "      <td>5.050331</td>\n",
       "      <td>4.869668</td>\n",
       "      <td>4.957764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.8</th>\n",
       "      <td>5.068364</td>\n",
       "      <td>5.027085</td>\n",
       "      <td>4.968997</td>\n",
       "      <td>4.956700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.8</th>\n",
       "      <td>5.059228</td>\n",
       "      <td>5.045101</td>\n",
       "      <td>4.942794</td>\n",
       "      <td>4.973249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11.5</th>\n",
       "      <td>5.094236</td>\n",
       "      <td>5.014571</td>\n",
       "      <td>5.100418</td>\n",
       "      <td>4.965507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.5</th>\n",
       "      <td>5.125666</td>\n",
       "      <td>5.064253</td>\n",
       "      <td>4.990459</td>\n",
       "      <td>4.989387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.5</th>\n",
       "      <td>5.178327</td>\n",
       "      <td>5.152544</td>\n",
       "      <td>5.135211</td>\n",
       "      <td>5.107515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.5</th>\n",
       "      <td>5.33835</td>\n",
       "      <td>5.254827</td>\n",
       "      <td>5.316797</td>\n",
       "      <td>5.223901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.5</th>\n",
       "      <td>5.21571</td>\n",
       "      <td>5.181931</td>\n",
       "      <td>5.340636</td>\n",
       "      <td>5.176754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rfr       etr  lightGBM     stack\n",
       "0.3   4.897571  4.926087    4.9499  4.880102\n",
       "0.8   4.812274  4.833755  4.743672  4.751666\n",
       "1.3   4.978854  4.951225  4.823021  4.860961\n",
       "1.8   4.940128  4.878684    4.7317  4.768909\n",
       "2.3   4.748331  4.741096  4.716704  4.657279\n",
       "2.8   4.974619  4.939024  4.701327  4.821513\n",
       "3.3   4.752634  4.768663  4.727463  4.689459\n",
       "3.8   4.801476  4.805669  4.717717  4.717100\n",
       "4.3   4.797783  4.818087  4.726396  4.733779\n",
       "4.8   5.012914  4.979691   4.82601  4.885367\n",
       "5.8   5.085126  4.950391   4.92746  4.857789\n",
       "6.8   4.838069   4.91594  4.716451  4.831464\n",
       "7.8   5.081228  5.050331  4.869668  4.957764\n",
       "8.8   5.068364  5.027085  4.968997  4.956700\n",
       "9.8   5.059228  5.045101  4.942794  4.973249\n",
       "11.5  5.094236  5.014571  5.100418  4.965507\n",
       "13.5  5.125666  5.064253  4.990459  4.989387\n",
       "15.5  5.178327  5.152544  5.135211  5.107515\n",
       "17.5   5.33835  5.254827  5.316797  5.223901\n",
       "19.5   5.21571  5.181931  5.340636  5.176754"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_components = 256\n",
    "num_epochs = 1000\n",
    "date = 1707\n",
    "name = 'id_rlgh3'\n",
    "swap_sp= pd.read_csv('swap_spec.csv',index_col = 0)  #/100\n",
    "swap_ph= pd.read_csv('swap_ph.csv',index_col = 0)\n",
    "rlgh3_spec= pd.read_csv('rlgh3.csv',index_col = 0)\n",
    "rlgh_spec= pd.read_csv('rlgh_spec.csv',index_col = 0)\n",
    "#pre_glove(mp20_spec,mp20_env,cores = mp20_core,no_components=64,data_name='em_64_con_mat_ea_1207',test_method = 'cv',glove_abuna = True,recon=True,pass_va = True)\n",
    "_ = pre_glove(swap_sp/100,swap_ph,cores = rlgh3_spec/100,emb_method = 'plus',stack = True,no_components=no_components,data_name='%s_ae%s_%s'%(name,no_components,date),test_method = 'cv',num_epochs= 1000,glove_abuna = True,recon=True,pass_va = False)\n",
    "_ = pre_glove(swap_sp/100,swap_ph,cores = rlgh3_spec/100,emb_method = 'only',stack = True,no_components=no_components,data_name='%s_e%s_%s'%(name,no_components,date),test_method = 'cv',num_epochs= 1000,glove_abuna = True,recon=True,pass_va = False)\n",
    "_ = pre_glove(swap_sp/100,swap_ph,cores = rlgh3_spec/100,emb_method = 'none',stack = True,no_components=1,data_name='%s_a%s_%s'%(name,no_components,date),test_method = 'cv',glove_abuna = True,recon=True,pass_va = False)\n",
    "name = 'id_rlgh'\n",
    "_ = pre_glove(swap_sp/100,swap_ph,cores = rlgh_spec/100,emb_method = 'plus',stack = True,no_components=no_components,data_name='%s_ae%s_%s'%(name,no_components,date),test_method = 'cv',num_epochs= 1000,glove_abuna = True,recon=True,pass_va = True)\n",
    "_ = pre_glove(swap_sp/100,swap_ph,cores = rlgh_spec/100,emb_method = 'only',stack = True,no_components=no_components,data_name='%s_e%s_%s'%(name,no_components,date),test_method = 'cv',num_epochs= 1000,glove_abuna = True,recon=True,pass_va = True)\n",
    "_ = pre_glove(swap_sp/100,swap_ph,cores = rlgh_spec/100,emb_method = 'none',stack = True,no_components=1,data_name='%s_a%s_%s'%(name,no_components,date),test_method = 'cv',glove_abuna = True,recon=True,pass_va = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b718c0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt38",
   "language": "python",
   "name": "pt38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
