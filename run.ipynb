{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf1b91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# Generate the communtity list\n",
    "\n",
    "def make_sents(sp_matrix):\n",
    "    sent = []\n",
    "    unfort_list = []\n",
    "    ii =0\n",
    "    for i in range(sp_matrix.shape[0]):  \n",
    "\n",
    "        pd_table_sub = sp_matrix.iloc[i,:]>0\n",
    "        c_v = sp_matrix.iloc[i,:][sp_matrix.iloc[i,:]>0]\n",
    "        if len(c_v)>1:\n",
    "            sent.append(c_v)\n",
    "        else:\n",
    "            unfort_list.append(ii)\n",
    "        ii += 1\n",
    "        print('\\r  %s...'%ii,end = '')\n",
    "    return(sent)\n",
    "\n",
    "\n",
    "\n",
    "# The main function for ecological baesd glove\n",
    "def glove_dic(sent,embed_size,vocab,wf,num_epochs = 5,glove_abuna = True):\n",
    "    # Set parameters\n",
    "    xmax = 2\n",
    "    alpha = 0.75\n",
    "    batch_size = 100\n",
    "    l_rate = 0.001\n",
    "    num_epochs = num_epochs\n",
    "\n",
    "    # Create vocabulary and word lists\n",
    "    word_list = sent\n",
    "    w_list_size = len(word_list)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Create word to index mapping\n",
    "    w_to_i = {word: ind for ind, word in enumerate(vocab)}\n",
    "    comat = np.zeros((vocab_size, vocab_size))\n",
    "    if glove_abuna == True:\n",
    "        # Construct co-occurence matrix\n",
    "        for i_init in range(w_list_size):\n",
    "            cc = sent[i_init]\n",
    "            for i in range(len(cc)):\n",
    "                for j in range(len(cc)):\n",
    "                    if i != j:\n",
    "                        ind = w_to_i[cc.index[i]]\n",
    "                        lind = w_to_i[cc.index[j]]\n",
    "                        comat[ind, lind] += cc.values[i]\n",
    "                        comat[lind,ind] += cc.values[j]\n",
    "    # Non-zero co-occurrences\n",
    "    else:\n",
    "        for i_init in range(w_list_size):\n",
    "            cc = sent[i_init]\n",
    "            for i in range(len(cc)):\n",
    "                for j in range(len(cc)):\n",
    "                    if i != j:\n",
    "                        ind = w_to_i[cc.index[i]]\n",
    "                        lind = w_to_i[cc.index[j]]\n",
    "                        comat[ind, lind] += 1\n",
    "                        comat[lind,ind] += 1\n",
    "\n",
    "    coocs = np.transpose(np.nonzero(comat))\n",
    "    # Weight function\n",
    "    # Set up word vectors and biases\n",
    "    l_embed, r_embed = [\n",
    "\t[Variable(torch.from_numpy(np.random.normal(0, 0.01, (embed_size, 1))),\n",
    "\t\trequires_grad = True) for j in range(vocab_size)] for i in range(2)]\n",
    "    l_biases, r_biases = [\n",
    "        [Variable(torch.from_numpy(np.random.normal(0, 0.01, 1)), \n",
    "            requires_grad = True) for j in range(vocab_size)] for i in range(2)]\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = optim.Adam(l_embed + r_embed + l_biases + r_biases, lr = l_rate)\n",
    "\n",
    "    # Batch sampling function\n",
    "    def gen_batch():\t\n",
    "        sample = np.random.choice(np.arange(len(coocs)), size=batch_size, replace=False)\n",
    "        l_vecs, r_vecs, covals, l_v_bias, r_v_bias = [], [], [], [], []\n",
    "        for chosen in sample:\n",
    "            ind = tuple(coocs[chosen])\n",
    "            l_vecs.append(l_embed[ind[0]])\n",
    "            r_vecs.append(r_embed[ind[1]])\n",
    "            covals.append(comat[ind])\n",
    "            l_v_bias.append(l_biases[ind[0]])\n",
    "            r_v_bias.append(r_biases[ind[1]])\n",
    "        return l_vecs, r_vecs, covals, l_v_bias, r_v_bias\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(num_epochs):\n",
    "        num_batches = int(w_list_size/batch_size)\n",
    "        avg_loss = 0.0\n",
    "        for batch in range(num_batches):\n",
    "            optimizer.zero_grad()\n",
    "            l_vecs, r_vecs, covals, l_v_bias, r_v_bias = gen_batch()\n",
    "            # For pytorch v2 use, .view(-1) in torch.dot here. Otherwise, no need to use .view(-1).\n",
    "            loss = sum([torch.mul((torch.dot(l_vecs[i].view(-1), r_vecs[i].view(-1)) +\n",
    "                    l_v_bias[i] + r_v_bias[i] - np.log(covals[i]))**2,\n",
    "                    wf(covals[i])) for i in range(batch_size)])\n",
    "            avg_loss += loss.data[0]/num_batches\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"\\r Average loss for epoch \"+str(epoch+1)+\": \", avg_loss.item(),end = '')\n",
    "    print(' ')\n",
    "    lem_list =[];rem_list=[];spname_list = []\n",
    "    for word_ind in range(len(vocab)):\n",
    "        # Create embedding by summing left and right embeddings\n",
    "        rem_list.append(l_embed[word_ind].data.numpy())  \n",
    "        spname_list.append(vocab[word_ind])\n",
    "    #cc = pd.DataFrame(np.array(lem_list)[:,:,0])\n",
    "    #cc['Species'] = spname_list\n",
    "    emb_dic = pd.DataFrame(np.array(rem_list)[:,:,0])\n",
    "    #emb_dic['Species'] = spname_list\n",
    "    emb_dic.index = vocab\n",
    "    \n",
    "    return(emb_dic)\n",
    "\n",
    "# Generate the communtity embedding\n",
    "\n",
    "def make_com_emb(sp_matrix,emb_dic,embed_size):\n",
    "    emb_sent = []\n",
    "    for i in range(sp_matrix.shape[0]):  \n",
    "\n",
    "        pd_table_sub = sp_matrix.iloc[i,:]#>0\n",
    "        c_v = pd_table_sub[pd_table_sub>0].index#np.unique(pd_table_sub.Species.dropna())\n",
    "        init_vec = np.zeros([embed_size])\n",
    "        for ii in c_v:\n",
    "            #i_sum += 1\n",
    "            init_vec += np.float32(emb_dic.loc[ii].values)\n",
    "\n",
    "        emb_sent.append(init_vec/len(c_v)) #/i_sum\n",
    "\n",
    "    #print('\\r  %s.../95104'%i,end = '')\n",
    "    return(np.array(emb_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77d37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc3eb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor,GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression,ARDRegression\n",
    "\n",
    "import argparse\n",
    "import pprint\n",
    "#import gensim\n",
    "from sklearn.model_selection import cross_validate,LeaveOneOut\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "import pickle\n",
    "import math\n",
    "#import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error,accuracy_score,recall_score\n",
    "import random\n",
    "def to_mae(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "def to_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "def to_mse(y_true, y_pred):\n",
    "    return (mean_squared_error(y_true, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab63ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The embedding name for printing importantce indicators\n",
    "def emb_names(table):\n",
    "    new_p_list =[]\n",
    "    for i in range(table.shape[1]):\n",
    "        new_p_list.append('em'+str(i))\n",
    "    return(new_p_list)\n",
    "\n",
    "# Keep the fossile data have the same taxon with normal data set\n",
    "def trans_to_core(spec,core):\n",
    "    new_pd = pd.DataFrame(np.zeros([core.shape[0], spec.shape[1]]))\n",
    "    new_pd.columns = spec.columns\n",
    "    for i_index,i_names in enumerate(new_pd.columns):\n",
    "        try:            \n",
    "            new_pd.iloc[:,i_index] = core[i_names].values\n",
    "        except:\n",
    "            continue\n",
    "    return(new_pd)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def to_pers(df):\n",
    "    row_sums = df.sum(axis=1)\n",
    "    # Divide each value in the row by the corresponding row sum and multiply by 100\n",
    "    df_percentage = df.div(row_sums, axis=0) \n",
    "    return(df_percentage)\n",
    "\n",
    "def gcd_many(s):\n",
    "    g = 0\n",
    "    for i in range(len(s)):\n",
    "        if i == 0:\n",
    "            g = s[i]\n",
    "        else:\n",
    "            g=math.gcd(g,s[i])\n",
    "    return g\n",
    "\n",
    "\n",
    "def to_num(matrix_init):\n",
    "    matrix = matrix_init.copy()\n",
    "    for i in range(matrix.shape[0]):\n",
    "        sub_raw = matrix.iloc[i,:].values\n",
    "        sub_raw_2 = sub_raw[sub_raw>0]\n",
    "        min_raw  = min(sub_raw_2)\n",
    "        sub_raw = sub_raw*10000#sub_raw/min_raw  ## 0/min_raw  = 0\n",
    "        sub_raw = sub_raw/gcd_many(sub_raw[sub_raw>0].astype('int'))\n",
    "    matrix.iloc[i,:] = sub_raw\n",
    "    return(matrix)\n",
    "\n",
    "\n",
    "# The main function for MEMLM\n",
    "def pre_glove(Biological_abundance_matrix,environmental_values_matrix,test_method = 'loo',no_components = 256, data_name='Test',\n",
    "              add_tr_env = [], cores = [],core_sp = [],stack = True, glove_abuna = True,random_no = 123,\n",
    "              recon=False,pass_va = False,save_vali = True,cor_emb = False, emb_method = 'plus',test_set = [],num_epochs = 300):\n",
    "    random_no = random_no\n",
    "    # set the flags\n",
    "    if len(test_set) >0:\n",
    "        recon= False\n",
    "        pass_va = False\n",
    "        cores = []\n",
    "        \n",
    "    if emb_method == 'only':\n",
    "        only_emb = True\n",
    "        only_abun = False\n",
    "        print('using the embedding matrix')\n",
    "    elif emb_method == 'none':\n",
    "        only_abun = True\n",
    "        only_emb = False\n",
    "        print('using the abundance matrix')\n",
    "    else:\n",
    "        only_abun = False\n",
    "        only_emb = False\n",
    "        print('using the embedding + abundance matrix')\n",
    "    # Set the random seeds\n",
    "    random.seed(random_no)\n",
    "    torch.manual_seed(random_no)\n",
    "    torch.cuda.manual_seed(random_no)  # If using CUDA\n",
    "    torch.manual_seed(random_no)\n",
    "\n",
    "    # Set the random seed for NumPy\n",
    "    np.random.seed(0)   \n",
    "        \n",
    "        \n",
    "        \n",
    "    pd_table = Biological_abundance_matrix\n",
    "    envs = environmental_values_matrix\n",
    "    #save_vali = save_vali            \n",
    "    if recon ==False:\n",
    "        cores = []\n",
    "    else:\n",
    "        #print(cores.shape)\n",
    "        cor_sp = (set(cores.columns)&set(pd_table.columns))\n",
    "    try:\n",
    "        (envs.shape[1])\n",
    "    except:\n",
    "        envs = pd.DataFrame(envs.T)\n",
    "    \n",
    "    #Statistics of species contained in each sample\n",
    "    print('scale: %s'%pd_table.iloc[0,:].sum())\n",
    "    xmax = 2 \n",
    "    alpha = 0.75\n",
    "    def wf(x):\n",
    "        return x\n",
    "    if glove_abuna == True:\n",
    "        if (pd_table.iloc[0,:].sum() > 1.3):\n",
    "            pd_table_2 = pd_table\n",
    "            #if np.min(pd_table_2[pd_table_2>0]).min()>1:\n",
    "            def wf(x):\n",
    "                if x < xmax:\n",
    "                    return (x/xmax)**alpha\n",
    "                return 1\n",
    "        else:\n",
    "            pd_table_2 = to_num(pd_table)\n",
    "\n",
    "    #pd_table_sub = pd_table.iloc[i,:]*250\n",
    "    if recon == True:\n",
    "        if cor_emb == True:\n",
    "            pd_table_2 = pd_table_2[cor_sp]\n",
    "    if only_abun == False:\n",
    "        sent = make_sents(pd_table)\n",
    "        print('creating the embedding now...')   ###\n",
    "    \n",
    "        embed_size = no_components\n",
    "        vocab = pd_table.columns\n",
    "        #embed_size = 256\n",
    "        emb_dic = glove_dic(sent,embed_size,vocab,wf,glove_abuna = glove_abuna,num_epochs = num_epochs)\n",
    "        \n",
    "        paragraph_list = make_com_emb(pd_table,emb_dic,embed_size)\n",
    "        emb_name_list = list(emb_names(paragraph_list))\n",
    "\n",
    "        #print('Species num(availble in asscoiation): %s' % len(corpus_model.dictionary))\n",
    "    print('Species num: %s' % pd_table.shape[1])\n",
    "    print('Samples num: %s' % pd_table.shape[0])\n",
    "    print(' ')\n",
    "    print('=================================')\n",
    "    print('training the embedding ......')\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    if only_emb == True:  \n",
    "        para_names =  emb_name_list   \n",
    "\n",
    "    elif only_abun == True:\n",
    "        para_names = pd_table.columns\n",
    "        paragraph_list = np.array(pd_table)\n",
    "    else:\n",
    "        print('adding the speiese abunadance now...')\n",
    "        paragraph_list = np.concatenate((np.array(paragraph_list),np.array(pd_table)), axis = 1)    ###\n",
    "        data_name = '%s_rich'%data_name\n",
    "        para_names =  emb_name_list + list(pd_table.columns)\n",
    "        #print([len(para_names),len(list(emb_names(np.array(paragraph_list)))),len(list(emb_names(np.array(paragraph_list))))])    \n",
    "        #print(paragraph_list.shape)\n",
    "        \n",
    "    if  len(add_tr_env) !=0:\n",
    "        data_name = '%s_added'%data_name\n",
    "        paragraph_list = np.concatenate((np.array(paragraph_list),np.array(add_tr_env)), axis = 1)\n",
    "        para_names = emb_name_list + list(add_tr_env.columns)\n",
    "        #print(paragraph_list.shape)\n",
    "    #print(len(para_names))\n",
    "\n",
    "        #print(paragraph_list.shape,cores.shape)\n",
    "    if pass_va == False :  \n",
    "        if test_method == 'cv':\n",
    "            cs=train_data_pre(paragraph_list,envs,data_name=data_name,test_method='cv',para_names = para_names, stack = stack, save_vali = save_vali)\n",
    "        elif test_method == 'loo':\n",
    "            cs=train_data_pre(paragraph_list,envs,data_name=data_name,test_method='loo',para_names = para_names, stack = stack, save_vali = save_vali)\n",
    "        elif test_method == 'diy':\n",
    "            cs=train_data_pre(paragraph_list,envs,data_name=data_name,test_method='diy',para_names = para_names, stack = stack,test_set=test_set)\n",
    "\n",
    "        else:\n",
    "            print('you shall choose one of these validate method:  cv, loo,diy') \n",
    "    if len(cores) !=0:\n",
    "        print('reconstructing the %s'%data_name)\n",
    "        cores_index = cores.index\n",
    "        if len(cores.columns) > len(pd_table.columns):      \n",
    "            cores = cores[cor_sp]\n",
    "            \n",
    "        elif len(cores.columns) < len(pd_table.columns):\n",
    "            cores = trans_to_core(pd_table,cores)\n",
    "        paragraph_list2 = []\n",
    "        #Generate embedding for each sample\n",
    "        cores.columns = pd_table.columns\n",
    "        if only_abun == False:\n",
    "            paragraph_list2 = make_com_emb(cores,emb_dic,embed_size)\n",
    "            if only_emb == False:\n",
    "                paragraph_list = np.concatenate((np.array(paragraph_list)[:,:no_components],np.array(pd_table)), axis = 1)\n",
    "                para_names = emb_name_list+list(pd_table.columns)\n",
    "                cores = np.concatenate((np.array(paragraph_list2),np.array(cores)), axis = 1)    #\n",
    "            else:\n",
    "                cores = np.array(paragraph_list2)\n",
    "        cs = recon_data(paragraph_list,envs,cores,cores_index,pass_va,data_name=data_name,trastacks = stack,test_method=test_method,para_names = para_names)\n",
    "        \n",
    "    print('The operation is over. Have a nice day!')\n",
    "    print('')\n",
    "    print('')\n",
    "\n",
    "    return(cs)\n",
    "\n",
    "def train_data_pre(X,y,data_name,GBR = False,n_estimators = 1000,test_method = 'loo',stack = True,save_vali = False,para_names=[],test_set = []):\n",
    "    if save_vali == True:\n",
    "        op_pirnt = 'on'\n",
    "    else:\n",
    "        op_pirnt = 'off'\n",
    "    print('save validation results option: %s'%op_pirnt)\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    acc_tt_list = []\n",
    "    try:\n",
    "        y.shape[1]\n",
    "        init_name = data_name\n",
    "        print('has multi environmental values')\n",
    "        for y_col in range(y.shape[1]):\n",
    "            pd_acc = pd.DataFrame(np.zeros([y.shape[1]*2,5]))\n",
    "            data_name = '%s_%s'%(init_name,y.columns[y_col])\n",
    "            X =np.array(X)\n",
    "            yy =np.array(y.iloc[:,y_col])\n",
    "            xx= X[~pd.isnull(yy)]\n",
    "            print('sub mission:  training %s now...'%data_name)\n",
    "            if test_method== 'loo':\n",
    "                acc = train_data_loo(X=xx,y = yy,GBR = False,data_name=data_name,n_estimators = 1000,stack = stack, save_vali = save_vali,test_method=test_method,para_names = para_names)\n",
    "            elif test_method== 'cv':\n",
    "                acc = train_data_cv(X=xx,y = yy,GBR = False,data_name=data_name,n_estimators = 1000,stack = stack, save_vali = save_vali,test_method=test_method,para_names = para_names)\n",
    "            elif test_method== 'diy':\n",
    "                return(train_data_diy(X=xx,y = yy,test_set = test_set, GBR = False,data_name=data_name,n_estimators = 1000,stack = stack, test_method=test_method,para_names = para_names))\n",
    "\n",
    "        acc_tt_list.append([data_name,*acc])  \n",
    "        return(acc_tt_list)\n",
    "    except:\n",
    "        if test_method == 'split':\n",
    "            return(train_data_split_across(X,y,GBR = False,data_name=data_name,n_estimators = 1000,stack = stack,test_method=test_method,para_names = para_names))\n",
    "        if test_method == 'loo':\n",
    "            return(train_data_loo(X,y,GBR = False,data_name = data_name,n_estimators = 1000,stack = stack, save_vali = save_vali,test_method=test_method,para_names = para_names))\n",
    "        elif test_method == 'cv':\n",
    "            return(train_data_cv(X,y,GBR = False,data_name=data_name,n_estimators = 1000,stack = stack, save_vali = save_vali,test_method=test_method,para_names = para_names))     \n",
    "        elif test_method== 'diy':\n",
    "            return(train_data_diy(X=xx,y = yy,test_set = test_set, GBR = False,data_name=data_name,n_estimators = 1000,stack = stack, test_method=test_method,para_names = para_names)\n",
    ")\n",
    "def train_data_cv(X,y,data_name,test_method = [],para_names = [],GBR = False,n_estimators = 1000,stack = True,save_vali = False):       \n",
    "    \n",
    "    base_list = np.array(range(X.shape[0]))\n",
    "    x_train_names = para_names\n",
    "    ran_index = np.random.permutation(base_list)\n",
    "    y= np.array(y)\n",
    "    X = np.array(X)\n",
    "    X = X[ran_index]\n",
    "    y =y[ran_index]\n",
    "    y_token = 'multi'\n",
    "    y_test_l = []\n",
    "    pre_l = []\n",
    "    kf = KFold(n_splits=5)\n",
    "\n",
    "    for train, test in kf.split(base_list):\n",
    "    \n",
    "        try:\n",
    "            x_train, x_test, y_train, y_test = X[train],X[test],y[train],y[test]\n",
    "        except:\n",
    "            x_train, x_test, y_train, y_test = X.loc(axis=0)[train],X.loc(axis=0)[test],y.loc(axis=0)[train],y.loc(axis=0)[test]        \n",
    "        #print(x_train.shape,y_train.shape)\n",
    "        predict3 = make_predict(x_train, x_test, y_train, y_test,test_method,data_name,x_train_names)\n",
    "        if stack == True:\n",
    "\n",
    "            predict3 = to_stack_2(x_train,y_train,predict3)\n",
    "\n",
    "        y_test_l.append(y_test.reshape(-1,1))\n",
    "        pre_l.append(predict3)\n",
    "    \n",
    "    pd_re = tail(pre_l,y_test_l,predict3,data_name,stack = stack,test_method ='cv',save_vali = save_vali)\n",
    "    return(pd_re) #[choose_predict(acc)] #,y_test, predict3,rfr\n",
    "\n",
    "\n",
    "\n",
    "def train_data_diy(X,y,test_set,data_name,test_method = [],para_names = [],GBR = False,n_estimators = 1000,stack = True):       \n",
    "\n",
    "    x_train_names = para_names\n",
    "    X =np.array(X)\n",
    "    y = np.array(y)\n",
    "    y_token = 'multi'\n",
    "    print('test',X.shape)\n",
    "\n",
    "    \n",
    "    x_train = X\n",
    "    x_test = y\n",
    "    y_test = []\n",
    "    y_train = test_set\n",
    "    #print('MEASURED     RECON       RICHNESS')\n",
    "\n",
    "    predict3 = make_predict(x_train, x_test, y_train, y_test,test_method,data_name,x_train_names,print_importa = False)\n",
    "    if stack == True:\n",
    "        #print(y_train.shape,x_train.shape,np.array(predict3).shape)\n",
    "\n",
    "        predict3 = to_stack_2(x_train,y_train,predict3)\n",
    "        #print('OK')\n",
    "    return(predict3) #[choose_predict(acc)] #,y_test, predict3,rfr\n",
    "\n",
    "def make_predict(x_train, x_test, y_train, y_test,data_name , test_method,x_train_names,n_estimators = 1000, GBR = False, print_importa = True):\n",
    "    \n",
    "    if len(x_train.shape) == 1:\n",
    "            x_train = x_train.reshape(-1, 1)\n",
    "            x_test = x_test.reshape(-1, 1)\n",
    "    if len(y_test)>0:\n",
    "        if len(y_test.shape) == 1:\n",
    "            y_train = y_train.reshape(-1, 1)\n",
    "            y_test = y_test.reshape(-1, 1)\n",
    "            y_token = 'single'\n",
    "    #print([x_train.shape,x_test.shape])\n",
    "    predict3 = three_models( x_train,y_train,x_test,test_method,data_name, x_train_names, print_importa = True,print_model = False)\n",
    "\n",
    "\n",
    "    return(predict3)\n",
    "        \n",
    "def to_stack(x_train,y_train,predict3):\n",
    "    rfr2,best_no, quit_lightGBM = mulit_model_stack2(x_train,y_train)\n",
    "    #print(rfr2,best_no, quit_lightGBM)\n",
    "    acc3_l = []\n",
    "    acc3_l.append(np.array(predict3)[0,:])\n",
    "    acc3_l.append(np.array(predict3)[1,:])\n",
    "    acc3_l.append(np.array(predict3)[2,:])\n",
    "    #print(acc3_l)\n",
    "    #print(predict3)\n",
    "    if quit_lightGBM == True:\n",
    "        predict3.append(rfr2.predict((np.array(acc3_l).T)[:,:1]).reshape(-1,1)\n",
    "                       )\n",
    "    else:\n",
    "        predict3.append(rfr2.predict(np.array(acc3_l).T).reshape(-1,1))\n",
    "    #print(predict3)\n",
    "    return(predict3)\n",
    "\n",
    "def to_stack_2(x_train,y_train,predict3):\n",
    "    rfr2,best_no, quit_lightGBM = mulit_model_stack2(x_train,y_train,test_method = 'cv')\n",
    "    predict3 = np.array(predict3).T   # (3,8)\n",
    "    acc3_l = []\n",
    "    acc3_l.append(predict3[:,0])\n",
    "    acc3_l.append(predict3[:,1])\n",
    "    acc3_l.append(predict3[:,2])\n",
    "\n",
    "    if quit_lightGBM == True:\n",
    "                predict3 = np.concatenate((predict3,rfr2.predict(np.array(acc3_l).T)[:,:1].reshape(-1,1)\n",
    "                                          ), axis = 1) \n",
    "    else:\n",
    "        predict3 = np.concatenate((predict3,rfr2.predict(np.array(acc3_l).T).reshape(-1,1)), axis = 1) \n",
    "    #print(predict3)\n",
    "    return(predict3)\n",
    "    \n",
    "    \n",
    "def train_data_loo(X,y,test_method, data_name,para_names,GBR = False,n_estimators = 1000, stack = False,save_vali = False):\n",
    "    y = np.array(y)\n",
    "    x_train_names = para_names\n",
    "    X =np.array(X)\n",
    "    y_token = 'multi'\n",
    "    X_no =range(X.shape[0])\n",
    "    loo = LeaveOneOut()\n",
    "    y_test_l = []\n",
    "    pre_l = []\n",
    "    print('MEASURED     RECON       RICHNESS')\n",
    "\n",
    "    # Loo cross_validate\n",
    "    for train, test in loo.split(X_no):\n",
    "\n",
    "        x_train, x_test, y_train, y_test = X[train],X[test],y[train],y[test]\n",
    "\n",
    "        #x_train, x_test, y_train, y_test = train_test_split( X,y, test_size=0.50, random_state=33)\n",
    "        \n",
    "        predict3 = make_predict(x_train, x_test, y_train, y_test,test_method,data_name,x_train_names)\n",
    "\n",
    "        if stack == True:\n",
    "            predict3 = to_stack(x_train,y_train,predict3)\n",
    "\n",
    "        y_test_l.append(y_test)\n",
    "        pre_l.append(predict3)\n",
    "        print(\"   %.3f      %.3f         %s\" %(pre_l[-1][0],y_test_l[-1][0],len(x_test[x_test>0])))\n",
    "    #acc = []\n",
    "    pd_re = tail(pre_l,y_test_l,predict3,data_name,stack = stack,test_method = 'loo',save_vali = save_vali)\n",
    "    \n",
    "    return(pd_re) #[choose_predict(acc)] #,y_test, predict3,rfr\n",
    "\n",
    "def tail(pre_l,y_test_l,predict3,data_name,stack = False,test_method = 'loo',save_vali = False):\n",
    "    if test_method == 'cv':\n",
    "        predict3 = np.vstack(pre_l)\n",
    "        #print(predict3.shape)\n",
    "    else:\n",
    "        predict3 = np.array(pre_l)\n",
    "    y_test =  np.vstack(y_test_l).reshape(-1, 1)\n",
    "\n",
    "    acc_r2 = []\n",
    "    acc_rmse = []\n",
    "    #print(y_test.shape,predict3.shape)\n",
    "    #if test_method == 'cv':\n",
    "    #predict3 = predict3.T\n",
    "    for i in range(predict3.shape[1]):\n",
    "\n",
    "        try:\n",
    "            acc_r2.append(r2_score(y_test, predict3[:,i].reshape(-1, 1)))\n",
    "            acc_rmse.append(to_rmse(y_test, predict3[:,i].reshape(-1, 1)))\n",
    "        except:\n",
    "            acc_r2.append(r2_score(y_test, predict3[:,i]))\n",
    "            acc_rmse.append(to_rmse(y_test, predict3[:,i]))\n",
    "    try:\n",
    "        acc_r2.append(r2_score(y_test, predict3.mean(1).reshape(-1, 1)))\n",
    "        acc_rmse.append(to_rmse(y_test, predict3.mean(1).reshape(-1, 1)))\n",
    "    except:\n",
    "        acc_r2.append(r2_score(y_test, predict3[:,i].mean(1)))\n",
    "        acc_rmse.append(to_rmse(y_test, predict3[:,i].mean(1)))\n",
    "    acc = [acc_r2,acc_rmse]\n",
    "      \n",
    "    print(\"___________result___________\")\n",
    "    print(\"r2_score\")\n",
    "    if stack == False:\n",
    "        print(\"   rfr       etr          lightGBM\")\n",
    "        print(\"   %.3f     %.3f        %.3f\" %(acc[0][0],acc[0][1],acc[0][2]))\n",
    "        print(\"RMSEP\")\n",
    "\n",
    "        print(\"   rfr       etr          lightGBM\")\n",
    "        print(\"   %.3f     %.3f        %.3f\" %(acc[1][0],acc[1][1],acc[1][2]))\n",
    "        va_cloumns = [\"rfr\"  ,     \"etr\",          \"lightGBM\",  \"true\"]\n",
    "    else:\n",
    "\n",
    "        print(\"   rfr       etr       lightGBM       stack       mean\")\n",
    "        print(\"   %.3f     %.3f        %.3f       %.3f       %.3f\" %(acc[0][0],acc[0][1],acc[0][2],acc[0][3],acc[0][4]))\n",
    "        print(\"RMSEP\")\n",
    "\n",
    "        print(\"   rfr       etr       lightGBM       stack       mean\")\n",
    "        print(\"   %.3f     %.3f        %.3f       %.3f       %.3f\" %(acc[1][0],acc[1][1],acc[1][2],acc[1][3],acc[1][4]))\n",
    "        va_cloumns = [\"rfr\"  ,     \"etr\",          \"lightGBM\",   \"stack\",'mean',\"true\"]\n",
    "    ##pickle.dump([predict3,y_test],open('./pkl/cross_val_%s.pkl'%(data_name),\"wb\"), protocol=3)\n",
    "    #print(predict3.shape,y_test.shape)\n",
    "\n",
    "\n",
    "    if save_vali == True:\n",
    "        try:\n",
    "            va_results = pd.DataFrame(np.concatenate((predict3[:,:],y_test),axis = 1))\n",
    "        except:\n",
    "            va_results = pd.DataFrame(np.concatenate((predict3[:,:,0],y_test),axis = 1))\n",
    "        va_results.columns = va_cloumns[:-1]\n",
    "        va_results.to_csv('vadata_%s_%s.csv'%(test_method,data_name))\n",
    "    #pickle.dump([predict3,y_test],open('./pkl/vali_%s_%s.pkl'%(test_method,data_name),\"wb\"), protocol=3)\n",
    "    #r2_score(y_test.reshape((-1, 1)), predict3[-1].reshape((-1, 1)))\n",
    "    #mo.score(y_test.reshape((-1, 1)), predict3[0]\n",
    "    model_num = predict3.shape[1]\n",
    "    pd_re = pd.DataFrame(acc)\n",
    "    pd_re.columns =  ['rfr','etr','lightGBM','stack','mean']#[:model_num]\n",
    "    pd_re.index = ['r2_score','rmsep']\n",
    "    pd_re.to_csv('varesult_%s_%s.csv'%(test_method,data_name))   ######\n",
    "    return(pd_re)\n",
    "\n",
    "\n",
    "\n",
    "def recon_data(paragraph_list,envs,cores,cores_index,pass_va,para_names,test_method, data_name,GBR =False,trastacks = True):\n",
    "    ## starting recon data process!\n",
    "    y_train = envs\n",
    "    x_train_names = para_names\n",
    "    x_train = np.array(paragraph_list)\n",
    "    x_test = cores\n",
    "    predict3 = []\n",
    "    \n",
    "    if len(y_train.shape) == 1:\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "        y_token = 'single'\n",
    "    if pass_va == False:\n",
    "        print_importa = False\n",
    "    else:\n",
    "        print_importa = True\n",
    "    predict3 = three_models( x_train,y_train,x_test,test_method,data_name,x_train_names, print_importa = print_importa,print_model = False)\n",
    "                 \n",
    "    ## starting stack!!\n",
    "    if trastacks == True:\n",
    "        model_num = 4\n",
    "        rfr2,best_no, quit_lightGBM = mulit_model_stack2(x_train,y_train)\n",
    "\n",
    "        acc3_l = []\n",
    "        acc3_l.append(np.array(predict3)[0,:])\n",
    "        acc3_l.append(np.array(predict3)[1,:])\n",
    "        acc3_l.append(np.array(predict3)[2,:])        #print(np.array(acc3_l).shape)\n",
    "        if quit_lightGBM == True:\n",
    "            predict3.append(rfr2.predict((np.array(acc3_l).T)[:,:1]).reshape(-1,1))\n",
    "        else:\n",
    "            predict3.append(rfr2.predict(np.array(acc3_l).T).reshape(-1,1)\n",
    "                           )\n",
    "    else:\n",
    "        model_num = 3\n",
    "        print('without stack process...')\n",
    "    model_name_list = []\n",
    "    for i in range(model_num):\n",
    "        if i == 3:\n",
    "            i = -1\n",
    "        model_name = ['rfr','etr','lightGBM','stack'][i]\n",
    "        pd.DataFrame(predict3).T\n",
    "        #pd.DataFrame(predict3[i]).to_csv('./recon/%s_%s.csv'%(data_name,model_name))\n",
    "        print('created the %s_%s model output'%(data_name,model_name))\n",
    "        model_name_list.append(model_name)\n",
    "    ## reconstructions has been done\n",
    "    if data_name != 'test':\n",
    "        t_pd = pd.DataFrame(predict3).T\n",
    "        t_pd.columns = model_name_list\n",
    "        if model_name_list[-1] =='stack':\n",
    "            #t_pd['stack'] =  t_pd['stack'].apply(lambda x: x.replace('[','').replace(']','')) \n",
    "            t_pd['stack'] =  t_pd['stack'].apply(lambda x: x[0]) \n",
    "            t_pd['stack'] = t_pd['stack'].astype(float)\n",
    "        t_pd.index = cores_index\n",
    "        t_pd.to_csv('%s_core.csv'%(data_name))\n",
    "    return(t_pd)\n",
    "\n",
    "\n",
    "\n",
    "def mulit_model_stack2(x_train,y_train,GBR = False,test_method = 'loo'):\n",
    "    \n",
    "    test4 = []\n",
    "    predict4 = []\n",
    "    predict5_l = []\n",
    "    best_no = []\n",
    "    acc_list = []\n",
    "    acc_list2 = []\n",
    "\n",
    "    x_train=np.array(x_train)  #\n",
    "    y_train = np.array(y_train)\n",
    "    kf = KFold(n_splits=5)\n",
    "\n",
    "    \n",
    "    base_list = np.array(range(y_train.shape[0]))\n",
    "    ran_index = np.random.permutation(base_list)\n",
    "    x_train = x_train[ran_index]\n",
    "    y_train =y_train[ran_index]\n",
    "    pre_l = []\n",
    "    for train, test in kf.split(ran_index):\n",
    "        predict5 =[]\n",
    "        \n",
    "        try:\n",
    "            x_train2, x_test2, y_train2, y_test2 = x_train[train],x_train[test],y_train[train],y_train[test]\n",
    "        except:\n",
    "            x_train2, x_test2, y_train2, y_test2 = x_train.loc(axis=0)[train],x_train.loc(axis=0)[test],y_train.loc(axis=0)[train],y_train.loc(axis=0)[test]\n",
    "            \n",
    "        predict3 = three_models( x_train2,y_train2,x_test2, print_importa = False,print_model = False)\n",
    "        #print('pred',np.array(predict3).shape)\n",
    "        test4.append(y_test2.reshape(-1,1))\n",
    "        predict4.append(np.array(predict3).T)\n",
    "        # predict3 (3, 4)\n",
    "        use_inf = True\n",
    "        if test_method == 'loo':\n",
    "                predict5.append(np.array(predict3)[0,:])\n",
    "                predict5.append(np.array(predict3)[1,:])\n",
    "                predict5.append(np.array(predict3)[2,:])        \n",
    "        else:\n",
    "            if use_inf == True:\n",
    "                predict5.append(np.array(predict3)[0,:])\n",
    "                predict5.append(np.array(predict3)[1,:])\n",
    "                predict5.append(np.array(predict3)[2,:])\n",
    "            else:\n",
    "                \n",
    "                predict5.append((np.array(predict3)[0,:]+np.array(predict3)[1,:]).T/2)\n",
    "                predict5.append((np.array(predict3)[0,:]+np.array(predict3)[2,:]).T/2)\n",
    "                predict5.append((np.array(predict3)[1,:]+np.array(predict3)[2,:]).T/2)\n",
    "                predict5.append((np.array(predict3)[1,:]+np.array(predict3)[2,:]+np.array(predict3)[0,:]).T/3)\n",
    "       \n",
    "        predict5_l.append(np.array(predict5).T)\n",
    "\n",
    "        acc2 = []\n",
    "        predict5= np.array(predict5).T\n",
    "\n",
    "        \n",
    "    rfr2 = LinearRegression() #ARDRegression() LinearRegression() \n",
    "    #n_estimators = 1000, min_sample_leaf10)\n",
    "    predict5_lt = np.vstack(predict5_l)\n",
    "    #print('pred2',predict5_lt.shape)\n",
    "\n",
    "    test_lt = np.vstack(test4)\n",
    "\n",
    "    quit_lightGBM = False\n",
    "\n",
    "    rfr2.fit(np.vstack(predict5_lt),test_lt) \n",
    "    return(rfr2,best_no,quit_lightGBM)\n",
    "\n",
    "def three_models(x_train,y_train,x_test,test_method =[],data_name=[],x_train_names=[],GBR =False, print_importa = True,print_model = False,n_estimators = 1000,random_no = 123):\n",
    "    from numpy import random\n",
    "    #print(x_train.shape,'three_models')\n",
    "    predict3 = []\n",
    "    rfr = RandomForestRegressor(n_estimators = 1000,n_jobs=-2, random_state = random.seed(random_no))#n_estimators , min_sample_leaf10)\n",
    "    # training\n",
    "    rfr.fit(x_train,y_train) #.reshape((-1, 1)),\n",
    "    # predict and save forecast result\n",
    "    predict3.append(rfr.predict(x_test)) # .reshape((-1, 1))\n",
    "\n",
    "\n",
    "    etr = ExtraTreesRegressor(n_estimators = n_estimators,n_jobs=-2, random_state = random.seed(random_no))\n",
    "    etr.fit(x_train,y_train) #.reshape((-1, 1)),\n",
    "    predict3.append(etr.predict(x_test))\n",
    "    if GBR == True:\n",
    "        gbr = GradientBoostingRegressor(n_estimators = n_estimators,n_jobs=-2, random_state = random.seed(random_no))\n",
    "        gbr.fit(x_train,y_train) #.reshape((-1, 1)),\n",
    "        gbr_y_predict = gbr.predict(x_test)\n",
    "        predict3.append(gbr.predict(x_test))\n",
    "        predict3 = np.array(predict3)\n",
    "        print('the 3rd model is GBR')\n",
    "\n",
    "        imp_table.columns = ['rfr','etr']\n",
    "    else:\n",
    "        gbm = LGBMRegressor(objective='regression', num_leaves=31, learning_rate=0.05, n_estimators=1000,n_jobs=-2,random_state = random.seed(random_no),verbose = -1)\n",
    "\n",
    "        gbm.fit(x_train,y_train) #.reshape((-1, 1)),\n",
    "        predict3.append(gbm.predict(x_test))\n",
    "        imp_table = pd.DataFrame([rfr.feature_importances_,etr.feature_importances_,gbm.feature_importances_]).T\n",
    "        imp_table.columns = ['rfr','etr','LightGBM']\n",
    "    if print_importa == True:\n",
    "        \n",
    "\n",
    "        if  len(x_train_names) >0:\n",
    "            imp_table.index = x_train_names\n",
    "        imp_table = imp_table.sort_values(['rfr'],ascending = False)\n",
    "        imp_table.to_csv('importances_%s_%s.csv'%(test_method,data_name))\n",
    "    if print_model == False:\n",
    "        return(predict3)\n",
    "    else:\n",
    "        return(predict3,rfr,etr,model3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f05063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the embedding + abundance matrix\n",
      "scale: 0.8427\n",
      "  167...creating the embedding now...\n",
      " Average loss for epoch 1000:  7.211855607312498 \n",
      "Species num: 277\n",
      "Samples num: 167\n",
      " \n",
      "=================================\n",
      "training the embedding ......\n",
      "adding the speiese abunadance now...\n",
      "reconstructing the rlgh_ae256_0101_rich\n",
      "created the rlgh_ae256_0101_rich_rfr model output\n",
      "created the rlgh_ae256_0101_rich_etr model output\n",
      "created the rlgh_ae256_0101_rich_lightGBM model output\n",
      "created the rlgh_ae256_0101_rich_stack model output\n",
      "The operation is over. Have a nice day!\n",
      "\n",
      "\n",
      "using the embedding matrix\n",
      "scale: 0.8427\n",
      "  167...creating the embedding now...\n",
      " Average loss for epoch 1000:  7.211855607312498 \n",
      "Species num: 277\n",
      "Samples num: 167\n",
      " \n",
      "=================================\n",
      "training the embedding ......\n",
      "reconstructing the rlgh_e256_0101\n",
      "created the rlgh_e256_0101_rfr model output\n",
      "created the rlgh_e256_0101_etr model output\n",
      "created the rlgh_e256_0101_lightGBM model output\n",
      "created the rlgh_e256_0101_stack model output\n",
      "The operation is over. Have a nice day!\n",
      "\n",
      "\n",
      "using the abundance matrix\n",
      "scale: 0.8427\n",
      "Species num: 277\n",
      "Samples num: 167\n",
      " \n",
      "=================================\n",
      "training the embedding ......\n",
      "reconstructing the rlgh_a256_0101\n",
      "created the rlgh_a256_0101_rfr model output\n",
      "created the rlgh_a256_0101_etr model output\n",
      "created the rlgh_a256_0101_lightGBM model output\n",
      "created the rlgh_a256_0101_stack model output\n",
      "The operation is over. Have a nice day!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_components = 256\n",
    "num_epochs = 1000\n",
    "date = '0101'\n",
    "name = 'rlgh'\n",
    "swap_sp= pd.read_csv('swap_spec.csv',index_col = 0)  #/100\n",
    "swap_ph= pd.read_csv('swap_ph.csv',index_col = 0)\n",
    "rlgh_spec= pd.read_csv('rlgh_spec.csv',index_col = 0)\n",
    "_ = pre_glove(swap_sp/100,swap_ph,cores = rlgh_spec/100,emb_method = 'plus',stack = True,no_components=no_components,data_name='%s_ae%s_%s'%(name,no_components,date),test_method = 'cv',num_epochs= 1000,glove_abuna = True,recon=True,pass_va = True)\n",
    "_ = pre_glove(swap_sp/100,swap_ph,cores = rlgh_spec/100,emb_method = 'only',stack = True,no_components=no_components,data_name='%s_e%s_%s'%(name,no_components,date),test_method = 'cv',num_epochs= 1000,glove_abuna = True,recon=True,pass_va = True)\n",
    "_ = pre_glove(swap_sp/100,swap_ph,cores = rlgh_spec/100,emb_method = 'none',stack = True,no_components=1,data_name='%s_a%s_%s'%(name,no_components,date),test_method = 'cv',glove_abuna = True,recon=True,pass_va = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b718c0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt38",
   "language": "python",
   "name": "pt38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
